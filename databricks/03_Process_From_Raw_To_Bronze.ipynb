{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bda7aa2-83fe-4acf-b256-a9946201d286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/samuel.barroscatarino@educ.sasserno.fr/musicstreamapp/databricks/01_Initialize_Setting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7234b3-8078-44d5-984d-07e61b84febf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2f386f-cd37-4388-b97a-9bd5974c755a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_bronze_stream(table_name):\n",
    "    # Reading streaming data from raw\n",
    "    df_raw = spark.readStream.table(f\"{catalog_name}.{raw_schema}.{table_name}\")\n",
    "    \n",
    "    # Cleaning the data and adding columns\n",
    "    df_cleaned = (df_raw.withColumn(\"timestamp\", F.from_unixtime(F.col(\"ts\")/1000)\n",
    "                                    .cast(T.TimestampType()))\n",
    "                    .withColumn(\"registration_time\", F.from_unixtime(F.col(\"registration\")/1000)\n",
    "                                .cast(T.TimestampType()))\n",
    "                    .withColumn(\"latitude\", F.col(\"lat\"))\n",
    "                    .withColumn(\"longitude\", F.col(\"lon\"))\n",
    "                    .withColumn(\"ingestion_date\", F.current_date())\n",
    "                    .withColumn(\"bronze_id\", F.expr(\"uuid()\")))\n",
    "\n",
    "    # Create the stream\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9b2bd5-e65b-4b2a-9d56-c6d70a8cf798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_bronze_stream(table_name):\n",
    "    # Create Bronze stream\n",
    "    bronze_stream_df = create_bronze_stream(table_name)\n",
    "\n",
    "    # Write to Unity Catalog table\n",
    "    return (bronze_stream_df.writeStream\n",
    "             .format(\"delta\")\n",
    "             .outputMode(\"append\")\n",
    "             .queryName(f\"Streaming_Raw_to_Bronze_{table_name}\")\n",
    "             .trigger(availableNow=True)\n",
    "             .option(\"checkpointLocation\", f\"{checkpoint_path}/{bronze_schema}/{table_name}/\")\n",
    "             .option(\"mergeSchema\", \"true\")\n",
    "             .toTable(f\"{catalog_name}.{bronze_schema}.{table_name}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e19ca0c-122c-4c38-ae8c-e1a9a0d2b019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start processing all bronze streams\n",
    "queries = []\n",
    "\n",
    "for table_name in list_tables:\n",
    "    print(f\"Starting bronze stream processing for {table_name}...\")\n",
    "    query = process_bronze_stream(table_name)\n",
    "    queries.append(query)\n",
    "    print(f\"Stream processing started for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22932db9-9ddb-4688-828a-9e003ddf9b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Monitor each query\n",
    "for i, (table_name) in enumerate(list_tables):\n",
    "    print(f\"\\nStatus for {table_name}:\")\n",
    "    print(queries[i].status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f8e175-4560-4f21-99eb-36e8bfccd8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example queries for each table\n",
    "for table_name in list_tables:\n",
    "    print(f\"\\nSample data from {table_name}:\")\n",
    "    display(spark.sql(f\"SELECT * FROM {catalog_name}.{bronze_schema}.{table_name} LIMIT 5\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3324414945037288,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Process_From_Raw_To_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
