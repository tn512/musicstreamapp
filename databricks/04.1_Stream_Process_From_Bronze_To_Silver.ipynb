{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf2dfc1-9b24-4d0a-bf76-45721cbd9921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/samuel.barroscatarino@educ.sasserno.fr/musicstreamapp/databricks/01_Initialize_Setting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e794fbcd-db36-481c-92d4-6e4076d6675b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Dimension: Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e401367-4ef7-4cd1-82b2-6fd26fec1cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process users with SCD Type 2 handling for level changes\n",
    "def process_users_scd2(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Create a temporary view of the incoming batch\n",
    "    batch_df.createOrReplaceGlobalTempView(\"users_batch\")\n",
    "\n",
    "    # Check if dim_users table exists\n",
    "    tables = (spark.sql(f\"SHOW TABLES IN {catalog_name}.{silver_schema}\")\n",
    "              .filter(F.col(\"tableName\") == \"dim_users\")\n",
    "              .collect())\n",
    "    if len(tables) == 0:\n",
    "        # First-time table creation that handles historical changes\n",
    "        # This implementation captures the history of changes already in the source data\n",
    "            \n",
    "        # Create a temporary view of the current batch\n",
    "        spark.sql(\"\"\"\n",
    "                  CREATE OR REPLACE TEMPORARY VIEW users_with_change_history AS\n",
    "                  WITH user_level_changes AS (\n",
    "                      -- Detect level changes by comparing with previous level\n",
    "                      SELECT\n",
    "                        userId, \n",
    "                        firstName,\n",
    "                        lastName,\n",
    "                        gender,\n",
    "                        level,\n",
    "                        registration_time,\n",
    "                        timestamp,\n",
    "                        LAG(level) OVER (PARTITION BY userId ORDER BY timestamp) AS prev_level,\n",
    "                        -- Flag rows where level changed or it's the first appearance of the user\n",
    "                        CASE \n",
    "                            WHEN LAG(level) OVER (PARTITION BY userId ORDER BY timestamp) IS NULL THEN 1\n",
    "                            WHEN level != LAG(level) OVER (PARTITION BY userId ORDER BY timestamp) THEN 1\n",
    "                            ELSE 0\n",
    "                        END AS level_changed\n",
    "                      FROM global_temp.users_batch\n",
    "                  ),\n",
    "                  change_groups AS (\n",
    "                      -- Assign a group number to each sequence of the same level\n",
    "                      SELECT \n",
    "                        *,\n",
    "                        SUM(level_changed) OVER (PARTITION BY userId ORDER BY timestamp) AS change_group\n",
    "                      FROM user_level_changes\n",
    "                  ),\n",
    "                  change_boundaries AS (\n",
    "                      -- Get the first and last timestamp for each level period\n",
    "                      SELECT \n",
    "                        userId,\n",
    "                        firstName,\n",
    "                        lastName,\n",
    "                        gender,\n",
    "                        level,\n",
    "                        change_group,\n",
    "                        MIN(registration_time) AS registration_time,\n",
    "                        MIN(timestamp) AS start_date,\n",
    "                        MAX(timestamp) AS end_date\n",
    "                      FROM change_groups\n",
    "                      GROUP BY userId, firstName, lastName, gender, level, change_group\n",
    "                  ),\n",
    "                  finalized_changes AS (\n",
    "                      -- Create proper activation and expiration dates for each record\n",
    "                      SELECT\n",
    "                        userId,\n",
    "                        firstName,\n",
    "                        lastName,\n",
    "                        gender,\n",
    "                        level,\n",
    "                        registration_time,\n",
    "                        CAST(date_trunc('day', start_date) AS DATE) AS row_activation_date,\n",
    "                        CASE\n",
    "                            WHEN LEAD(start_date) OVER (PARTITION BY userId ORDER BY start_date) IS NOT NULL\n",
    "                            THEN CAST(date_trunc('day', LEAD(start_date) OVER (PARTITION BY userId ORDER BY start_date)) AS DATE)\n",
    "                            ELSE CAST('9999-12-31' AS DATE)\n",
    "                        END AS row_expiration_date,\n",
    "                        CASE\n",
    "                            WHEN LEAD(start_date) OVER (PARTITION BY userId ORDER BY start_date) IS NULL THEN 1\n",
    "                            ELSE 0\n",
    "                        END AS current_row,\n",
    "                        md5(concat_ws('|', userId, level, start_date)) AS userKey\n",
    "                      FROM change_boundaries\n",
    "                  )\n",
    "                  SELECT * FROM finalized_changes\n",
    "                  ORDER BY userId, row_activation_date\n",
    "                  \"\"\")\n",
    "        \n",
    "        # Write the SCD2 data to the dimension table\n",
    "        (spark.table(\"users_with_change_history\").write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .saveAsTable(f\"{catalog_name}.{silver_schema}.dim_users\"))\n",
    "        \n",
    "        print(\"Initialized dim_users table with historical changes\")\n",
    "    \n",
    "    else:\n",
    "        # 1. Find users with level changes\n",
    "        spark.sql(\"\"\"\n",
    "                  CREATE OR REPLACE TEMPORARY VIEW level_changes AS\n",
    "                  SELECT\n",
    "                    b.userId, \n",
    "                    b.firstName, \n",
    "                    b.lastName, \n",
    "                    b.gender, \n",
    "                    b.level AS new_level, \n",
    "                    b.registration_time,\n",
    "                    b.timestamp,\n",
    "                    d.level AS old_level,\n",
    "                    d.userKey,\n",
    "                    d.row_activation_date\n",
    "                  FROM global_temp.users_batch b \n",
    "                  JOIN music_streaming.silver.dim_users d \n",
    "                  ON b.userId = d.userId and d.current_row = 1\n",
    "                  WHERE b.level <> d.level\n",
    "                  \"\"\")\n",
    "        \n",
    "        # 2. Expire the current records for changed users\n",
    "        spark.sql(\"\"\"\n",
    "                  MERGE INTO music_streaming.silver.dim_users d\n",
    "                  USING level_changes c\n",
    "                  ON d.userKey = c.userKey and d.current_row = 1\n",
    "                  WHEN MATCHED THEN\n",
    "                    UPDATE SET current_row = 0, \n",
    "                               row_expiration_date = CAST(date_trunc('day', c.timestamp) AS DATE)\n",
    "                  \"\"\")\n",
    "        \n",
    "        # 3. Insert new records for users with level changes\n",
    "        spark.sql(\"\"\"\n",
    "                  INSERT INTO music_streaming.silver.dim_users\n",
    "                  SELECT \n",
    "                    userId,\n",
    "                    firstName,\n",
    "                    lastName,\n",
    "                    gender,\n",
    "                    new_level AS level,\n",
    "                    registration_time,\n",
    "                    CAST(date_trunc('day', timestamp) AS DATE) AS row_activation_date,\n",
    "                    CAST('9999-12-31' AS DATE) AS row_expiration_date,\n",
    "                    1 AS current_row,\n",
    "                    md5(concat_ws('|', userId, new_level, timestamp)) AS userKey\n",
    "                  FROM level_changes\n",
    "                  \"\"\")\n",
    "        \n",
    "        # 4. Insert completely new users (not in the dimension yet)\n",
    "        spark.sql(\"\"\"\n",
    "                  INSERT INTO music_streaming.silver.dim_users\n",
    "                  SELECT\n",
    "                    b.userId,\n",
    "                    b.firstName,\n",
    "                    b.lastName,\n",
    "                    b.gender,\n",
    "                    b.level,\n",
    "                    b.registration_time,\n",
    "                    CAST(date_trunc('day', b.timestamp) AS DATE) AS row_activation_date,\n",
    "                    CAST('9999-12-31' AS DATE) AS row_expiration_date,\n",
    "                    1 AS current_row,\n",
    "                    md5(concat_ws('|', b.userId, b.level, b.timestamp)) AS userKey\n",
    "                  FROM global_temp.users_batch b\n",
    "                  LEFT JOIN music_streaming.silver.dim_users d\n",
    "                  ON b.userId = d.userId\n",
    "                  WHERE d.userId IS NULL\n",
    "                  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75715944-24d1-4231-aacd-b54dc57c9a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use foreachBatch to implement SCD Type 2 logic for the users dimension\n",
    "df_users_stream = (spark.readStream.table(f\"{catalog_name}.{bronze_schema}.listen_events\")\n",
    "                   .select(\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\", \"registration_time\", \"timestamp\")\n",
    "                   .filter(F.col(\"userId\").isNotNull())\n",
    "                   .withColumn(\"firstName\", F.coalesce(F.col(\"firstName\"), F.lit(\"Unknown\")))\n",
    "                   .withColumn(\"lastName\", F.coalesce(F.col(\"lastName\"), F.lit(\"Unknown\")))\n",
    "                   .withColumn(\"gender\", F.coalesce(F.col(\"gender\"), F.lit(\"Unknown\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7985ee6-7efd-409e-913a-4f9c493c6396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write dim_users to Silver layer with streaming, using foreachBatch to handle SCD2\n",
    "dim_users = (df_users_stream.writeStream\n",
    "             .foreachBatch(process_users_scd2)\n",
    "             .option(\"checkpointLocation\", f\"{checkpoint_path}/{silver_schema}/dim_users/\")\n",
    "             .trigger(availableNow=True)\n",
    "             .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49e5ddc-9aaa-450e-be98-c4b487df6174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM music_streaming.silver.dim_users LIMIT 5;\n",
    "SELECT * FROM music_streaming.silver.dim_users WHERE current_row = 1 LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b189ab-d2b8-4d85-80d4-7bb6289ebc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH user_level_changes AS (\n",
    "    -- Detect level changes by comparing with previous level\n",
    "    SELECT\n",
    "      userId, \n",
    "      firstName,\n",
    "      lastName,\n",
    "      gender,\n",
    "      level,\n",
    "      registration_time,\n",
    "      timestamp,\n",
    "      LAG(level) OVER (PARTITION BY userId ORDER BY timestamp) AS prev_level,\n",
    "      -- Flag rows where level changed or it's the first appearance of the user\n",
    "      CASE \n",
    "          WHEN LAG(level) OVER (PARTITION BY userId ORDER BY timestamp) IS NULL THEN 1\n",
    "          WHEN level != LAG(level) OVER (PARTITION BY userId ORDER BY timestamp) THEN 1\n",
    "          ELSE 0\n",
    "      END AS level_changed\n",
    "    FROM (SELECT * FROM music_streaming.bronze.listen_events)\n",
    ") SELECT * FROM user_level_changes;\n",
    "\n",
    "/*\n",
    ",change_groups AS (\n",
    "    -- Assign a group number to each sequence of the same level\n",
    "    SELECT \n",
    "      *,\n",
    "      SUM(level_changed) OVER (PARTITION BY userId ORDER BY timestamp) AS change_group\n",
    "    FROM user_level_changes\n",
    "),\n",
    "change_boundaries AS (\n",
    "    -- Get the first and last timestamp for each level period\n",
    "    SELECT \n",
    "      userId,\n",
    "      firstName,\n",
    "      lastName,\n",
    "      gender,\n",
    "      level,\n",
    "      registration_time,\n",
    "      change_group,\n",
    "      MIN(timestamp) AS start_date,\n",
    "      MAX(timestamp) AS end_date\n",
    "    FROM change_groups\n",
    "    GROUP BY userId, firstName, lastName, gender, level, registration_time, change_group\n",
    "),\n",
    "finalized_changes AS (\n",
    "    -- Create proper activation and expiration dates for each record\n",
    "    SELECT\n",
    "      userId,\n",
    "      firstName,\n",
    "      lastName,\n",
    "      gender,\n",
    "      level,\n",
    "      registration_time,\n",
    "      CAST(date_trunc('day', start_date) AS DATE) AS row_activation_date,\n",
    "      CASE\n",
    "          WHEN LEAD(start_date) OVER (PARTITION BY userId ORDER BY start_date) IS NOT NULL\n",
    "          THEN CAST(date_trunc('day', LEAD(start_date) OVER (PARTITION BY userId ORDER BY start_date)) AS DATE)\n",
    "          ELSE CAST('9999-12-31' AS DATE)\n",
    "      END AS row_expiration_date,\n",
    "      CASE\n",
    "          WHEN LEAD(start_date) OVER (PARTITION BY userId ORDER BY start_date) IS NULL THEN 1\n",
    "          ELSE 0\n",
    "      END AS current_row,\n",
    "      md5(concat_ws('|', userId, level, start_date)) AS userKey\n",
    "    FROM change_boundaries\n",
    ")\n",
    "SELECT * FROM finalized_changes\n",
    "ORDER BY userId, row_activation_date*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48436843-63e2-409e-81e3-5c1f7e63fd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Dimension: Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b33fbaf-0f47-4bb8-bf99-73aa3fb52dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location_stream = (spark.readStream.table(f\"{catalog_name}.{bronze_schema}.listen_events\")\n",
    "                .select(\"city\", F.col(\"state\").alias(\"stateCode\"), \"zip\", \"latitude\", \"longitude\")\n",
    "                .filter(F.col(\"city\").isNotNull() &\n",
    "                        F.col(\"stateCode\").isNotNull())\n",
    "                .dropDuplicates([\"city\", \"stateCode\", \"zip\", \"latitude\", \"longitude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6130b13-4713-4b88-a836-3688c16626a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state_raw = spark.read.csv(f\"{raw_path}/state_codes.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70989415-132d-4cac-a218-95c28f641ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location_raw = (location_stream.join(state_raw, \"stateCode\", \"left\")\n",
    "                .withColumn(\"stateCode\", F.coalesce(F.col(\"stateCode\"), F.lit(\"NA\")))\n",
    "                .withColumn(\"stateName\", F.coalesce(F.col(\"stateName\"), F.lit(\"NA\")))\n",
    "                .withColumn(\"locationKey\", F.md5(F.concat_ws(\"|\", \n",
    "                    F.col(\"city\"), \n",
    "                    F.col(\"stateCode\"), \n",
    "                    F.coalesce(F.col(\"zip\"), F.lit(\"Unknown\"))\n",
    "                ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d542692-eb8a-458d-8c90-1c7775ee3ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(location_raw.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}/{silver_schema}/dim_location/\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(f\"{catalog_name}.{silver_schema}.dim_location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a90dd2-40d5-48c5-af5d-eabb037e3752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM music_streaming.silver.dim_location LIMIT 5;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3831845186719744,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.1_Stream_Process_From_Bronze_To_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
