{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a8ac7f-7e29-4296-9783-14cb7c2ccaac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/samuel.barroscatarino@educ.sasserno.fr/musicstreamapp/databricks/01_Initialize_Setting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce8816bb-0394-477f-af33-1f899f6291fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3886704-0533-4af3-bf4e-dc636fefeaa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kafka connection parameters\n",
    "kafka_bootstrap_servers = \"172.212.37.12:9092\"\n",
    "\n",
    "# Unity Catalog paths\n",
    "catalog_name = \"music_streaming\"\n",
    "schema_name = \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581dee53-8419-49a5-b504-f015d7a82d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schemas for each event type\n",
    "base_schema = StructType([\n",
    "    StructField(\"ts\", LongType(), True),\n",
    "    StructField(\"sessionId\", StringType(), True),\n",
    "    StructField(\"userId\", StringType(), True),\n",
    "    StructField(\"auth\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"itemInSession\", LongType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"userAgent\", StringType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"registration\", LongType(), True)\n",
    "])\n",
    "\n",
    "page_view_schema = StructType(base_schema.fields + [\n",
    "    StructField(\"page\", StringType(), True),\n",
    "    StructField(\"method\", StringType(), True),\n",
    "    StructField(\"status\", LongType(), True)\n",
    "])\n",
    "\n",
    "listen_schema = StructType(base_schema.fields + [\n",
    "    StructField(\"artist\", StringType(), True),\n",
    "    StructField(\"song\", StringType(), True),\n",
    "    StructField(\"duration\", LongType(), True)\n",
    "])\n",
    "\n",
    "auth_schema = StructType(base_schema.fields + [\n",
    "    StructField(\"success\", BooleanType(), True),\n",
    "    StructField(\"method\", StringType(), True),\n",
    "    StructField(\"status\", LongType(), True)\n",
    "])\n",
    "\n",
    "status_change_schema = StructType(base_schema.fields + [\n",
    "    StructField(\"prevLevel\", StringType(), True),\n",
    "    StructField(\"method\", StringType(), True),\n",
    "    StructField(\"status\", LongType(), True),\n",
    "    StructField(\"statusChangeType\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cdfa57-fdb4-4cc7-90fd-46dda09000b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Map topics to their schemas\n",
    "schemas = {\n",
    "    \"page_view_events\": page_view_schema,\n",
    "    \"listen_events\": listen_schema,\n",
    "    \"auth_events\": auth_schema,\n",
    "    \"status_change_events\": status_change_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667228be-9390-4f2d-ab31-8e4d1e6466bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_kafka_stream(topic):\n",
    "    \"\"\"Create a Kafka stream for a given topic with settings optimized for batch jobs.\"\"\"\n",
    "    # Check if checkpoint exists\n",
    "    try:\n",
    "        dbutils.fs.ls(f\"{checkpoint_path}/raw/{topic}/\")\n",
    "        checkpoint_exists = True\n",
    "    except:\n",
    "        checkpoint_exists = False\n",
    "    \n",
    "    # Set starting offset based on checkpoint existence\n",
    "    # For first run: use \"earliest\" to get all historical data\n",
    "    # For subsequent runs: rely on checkpoint (no need to specify startingOffsets)\n",
    "    kafka_options = {\n",
    "        \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "        \"subscribe\": topic,\n",
    "        \"failOnDataLoss\": \"false\",\n",
    "        \"kafka.security.protocol\": \"PLAINTEXT\"\n",
    "    }\n",
    "\n",
    "    # Only add startingOffsets for first run\n",
    "    if not checkpoint_exists:\n",
    "        kafka_options[\"startingOffsets\"] = \"earliest\"\n",
    "    \n",
    "    # Create the stream\n",
    "    return (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**kafka_options)\n",
    "            .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d005af9-1dc8-4c67-9619-43fb7629a6b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_kafka_stream(topic, schema):\n",
    "    \"\"\"Process a Kafka stream and write to Unity Catalog table.\"\"\"\n",
    "    # Create Kafka stream\n",
    "    kafka_stream_df = create_kafka_stream(topic)\n",
    "\n",
    "    # Parse the value column from Kafka which contains the JSON data\n",
    "    parsed_df = kafka_stream_df.select(\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
    "        col(\"topic\").alias(\"kafka_topic\"),\n",
    "        col(\"partition\").alias(\"kafka_partition\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "    ).select(\n",
    "        \"kafka_timestamp\", \n",
    "        \"kafka_topic\", \n",
    "        \"kafka_partition\", \n",
    "        \"kafka_offset\",\n",
    "        \"data.*\"\n",
    "    )\n",
    "\n",
    "    # Add ingestion timestamp\n",
    "    parsed_df = parsed_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "    # Write to Unity Catalog table\n",
    "    return(parsed_df.writeStream\n",
    "             .format(\"delta\")\n",
    "             .outputMode(\"append\")\n",
    "             .queryName(f\"Streaming_Kafka_to_Raw_{topic}\")\n",
    "             .trigger(availableNow=True)\n",
    "             .option(\"checkpointLocation\", f\"{checkpoint_path}/raw/{topic}/\")\n",
    "             .option(\"mergeSchema\", \"true\")\n",
    "             .toTable(f\"{catalog_name}.{schema_name}.{topic}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24437e45-11ba-4d86-b95d-27db2b1fbe76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start processing all streams\n",
    "queries = []\n",
    "\n",
    "for topic in list_tables:\n",
    "    print(f\"Starting stream processing for {topic}...\")\n",
    "    schema = schemas[topic]\n",
    "    query = process_kafka_stream(topic, schema)\n",
    "    queries.append(query)\n",
    "    print(f\"Stream processing started for {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ba1cfb-b473-4975-8434-f3ee3be972be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Monitor each query\n",
    "for i, (topic) in enumerate(list_tables):\n",
    "    print(f\"\\nStatus for {topic}:\")\n",
    "    print(queries[i].status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d90ba6-edff-4963-bdb9-89d354fcc9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example queries for each table\n",
    "for topic in list_tables:\n",
    "    print(f\"\\nSample data from {topic}:\")\n",
    "    display(spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.{topic} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce725bc-4efd-4996-8417-087ec5901e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example queries for each table\n",
    "for topic in list_tables:\n",
    "    print(f\"\\nSample data from {topic}:\")\n",
    "    display(spark.sql(f\"SELECT COUNT(1) FROM {catalog_name}.{schema_name}.{topic}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "849dc5e0-eb0a-47c9-a3b9-1e9e2d6657be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop all streams\n",
    "for i, (topic) in enumerate(list_tables):\n",
    "    print(f\"Stopping stream for {topic}...\")\n",
    "    queries[i].stop()\n",
    "    print(f\"Stream stopped for {topic}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Ingest_From_Kafka_To_Raw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
